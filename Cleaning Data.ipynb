{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import io\n",
    "import requests\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import re\n",
    "# from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('interview-data.csv')\n",
    "raw_names = df['plaid_dirty_name']\n",
    "unique_names = df.plaid_dirty_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_names2 = []\n",
    "for name in raw_names:\n",
    "#     name = name.replace\n",
    "    tempName = re.sub('[^a-zA-Z0-9\\n\\.]', ' ', name)\n",
    "    tempName = tempName.replace('.', ' ')\n",
    "    raw_names2.append(tempName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZING DATA\n",
    "tokenized_sents = [word_tokenize(name) for name in raw_names2]\n",
    "total_tokens = 0\n",
    "\n",
    "# REPLACING NUMBERS AND DIGITS WITH STRING \"NUM\"\n",
    "for i, words in enumerate(tokenized_sents):\n",
    "    total_tokens += len(words)\n",
    "    for j, word in enumerate(words):\n",
    "        if word.isnumeric():\n",
    "            tokenized_sents[i][j] = 'NUM'\n",
    "        else:\n",
    "            tokenized_sents[i][j] = word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREQUENCY OF WORD IN WHOLE DATASET\n",
    "countDict = Counter(chain.from_iterable(tokenized_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []                     # unique set of words\n",
    "word_count = []                # count of each unique word in whole doc\n",
    "for word in countDict:\n",
    "    words.append(word)\n",
    "    word_count.append(countDict[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = []                        # term frequency\n",
    "for sentence in tokenized_sents:\n",
    "    tempSent = []\n",
    "    for word in words:\n",
    "        tempSent.append(sentence.count(word))\n",
    "    tf.append(tempSent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docFreq = [0]*len(words)       # document frequency of each word\n",
    "for sent in tokenized_sents:\n",
    "    for i, word in enumerate(words):\n",
    "        if word in sent:\n",
    "            docFreq[i] += 1\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = []                       # inverse document frequency of each word\n",
    "for ele in docFreq:\n",
    "    idf.append(len(tokenized_sents)/ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = []                    # term freq * idf\n",
    "for i,sentences in enumerate(tf):\n",
    "    tempList = []\n",
    "    for j,word in enumerate(sentences):\n",
    "        tempList.append(tf[i][j]*docFreq[j])\n",
    "    tf_idf.append(tempList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity of each merchant name with every other merchant name\n",
    "cosine_similarity_matrix = np.zeros(shape=(len(tokenized_sents),len(tokenized_sents)))\n",
    "for i in range(len(tf_idf)-1):\n",
    "    for j in range(i+1, len(tf_idf)):\n",
    "        cosine = 1 - scipy.spatial.distance.cosine(tf_idf[i], tf_idf[j])\n",
    "        cosine_similarity_matrix[i][j] = cosine\n",
    "        cosine_similarity_matrix[j][i] = cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding set of unique merchant names based on the threshold cosine similarity value\n",
    "threshold = 0.9\n",
    "unique_merchants = []\n",
    "for i, sentence in enumerate(tokenized_sents):\n",
    "    insert = 1\n",
    "    if i>0:\n",
    "        for j, sent2 in enumerate(cosine_similarity_matrix[i]):\n",
    "            if j < i and sent2 > threshold:\n",
    "                insert = 0\n",
    "                break\n",
    "    if (insert!=0):\n",
    "        unique_merchants.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENSIZE DATA\n",
    "# CHANGE DIGITS TO \"NUM\" STRING\n",
    "# FIND TF-IDF\n",
    "# FIND CORRELATION BETWEEN SENTENCES\n",
    "# CORRELATION  > 0.8 -> SIMILAR SENTENCES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
